apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: artemis-broker
spec:
  # version: 2.32.0 # uncomment the version, in the hope that the broker chooses the most recent one!
  resourceTemplates:
      # The new AWS Load Balancer manages AWS Elastic Load Balancers for a Kubernetes cluster.
      # The controller provisions an AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress and
      # an AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer using IP targets on 1.18 or later Amazon EKS clusters.

      # Possible variables in resourceTemplates:
      # ITEM_NAME,
      # CR_NAME = artemis-broker,
      # BROKER_ORDINAL, CR_NAMESPACE, INGRESS_DOMAIN, RES_NAME
      # see https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-1-service-and-ingress-resources/
      # see https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-2-aws-load-balancer-controller/
      # https://aws.amazon.com/it/blogs/containers/setting-up-end-to-end-tls-encryption-on-amazon-eks-with-the-new-aws-load-balancer-controller/
      - selector:
          kind: "Ingress"
        # See list of ingress annotations:
        # https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/
        # Configuration of Application Load Balancer for Ingress
        annotations:
            alb.ingress.kubernetes.io/load-balancer-name: artemis-webconsole-lb-$(BROKER_ORDINAL)
            # options:
            # ip: target is specified using the ip address
            # instance: target is specified using the ID of the EC2 instance
            # instance is mainly used when backwards compatability is required or if it's required to route traffic
            # through an EC2 instance
            alb.ingress.kubernetes.io/target-type: ip
            # Starting v2.2.0 release, the AWS Load balancer controller provisions an internal NLB by default.
            # To create an internet-facing load balancer, apply the following annotation to your service:
            alb.ingress.kubernetes.io/scheme: internet-facing
            alb.ingress.kubernetes.io/healthcheck-path: /healthz
            # see https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/#ssl
            alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:eu-central-1:069512370968:certificate/e247b2ec-f880-42a3-9593-18f19dfbf3ca
            alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
            alb.ingress.kubernetes.io/tls-redirect: '443'
        patch:
            kind: Ingress
            spec:
                ingressClassName: alb
      # see https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html
      - selector:
          kind: "Service"
        # Configuration of Network Load Balancer for the acceptor
        annotations:
            service.beta.kubernetes.io/aws-load-balancer-name: anevis-acceptor-all-$(BROKER_ORDINAL)
            # this is telling k8s 'let AWS handle this load balancer in the cloud' (AWS Load Balancer Controller)
            # options: nlb-ip = deprecated
            # external = depend on aws-load-balancer-nlb-target-type
            service.beta.kubernetes.io/aws-load-balancer-type: external
            # see above
            service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
            # see above
            service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
            # This is the same format as above with the Ingress (Used for testing)
            # service.beta.kubernetes.io/certificate-arn: arn:aws:acm:eu-central-1:069512370968:certificate/e247b2ec-f880-42a3-9593-18f19dfbf3ca
            # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:eu-central-1:069512370968:certificate/e247b2ec-f880-42a3-9593-18f19dfbf3ca
            # .../aws-load-balancer-backend-protocol specifies whether to use TLS for the backend traffic between the load balancer and the kubernetes pods.
            # TCP = default
            # SSL = NLB uses TLS connections for the traffic to your kubernetes pods in case of TLS listeners
            service.beta.kubernetes.io/aws-load-balancer-backend-protocol: TCP
#             # In case of TCP, NLB with IP targets does not pass the client source IP address unless specifically configured via target group attributes.
#             # Your application pods might not see the actual client IP address even if NLB passes it along, for example instance mode with externalTrafficPolicy set to Cluster.
#             # In such cases, you can configure NLB proxy protocol v2 via annotation if you need visibility into the client source IP address on your application pods.
#             service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
      - selector:
          kind: "Service"
        patch:
            kind: Service
            spec:
                type: LoadBalancer
                # https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/annotations/#service-annotations
                # for k8s 1.22 and later if ... is set to false, NodePort must be allocated manually
                allocateLoadBalancerNodePorts: true
                # https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/annotations/#service-annotations
                # If you configure spec.loadBalancerClass, the controller defaults to instance target type
                loadBalancerClass: service.k8s.aws/nlb
  deploymentPlan:
    size: 3
    # (see description in operator.md)
    # = create brokers with persistent Storage, each broker requires 2GB
    # persistenceEnabled requires persistent volumes in the cluster
    #           Error with persistenceEnabled true: 0/3 nodes are available: pod has unbound immediate
    #           PersistentVolumeClaims. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.
    #           -> solved with podSecurityContext.fsGroup
    persistenceEnabled: true
    # <!-- -->
    # messageMigration requires 2+ brokers
    messageMigration: true
    # This enables to use the persistenceEnabled attribute above
    # PodSecurityContext defines the security settings for a pod and all of it's containers
    # fsGroup   special supplemental group where all the containers in a pod belong to. If a container writes a file,
    #           if a container writes a file, the file is created with the GID from fsGroup
    #           Any files created on a mounted volume are owned by the group specified in fsGroup
    #           -> This helps where multiple processes or containers share access to a file/volume
    #           !fsGroup applies to all containers within a pod!
    #           Setting fsGroup: 0 implies that all files created will be owned by the root group
    # -> https://github.com/artemiscloud/activemq-artemis-operator/issues/187 This provided the fix
    # Further discussion here: https://github.com/artemiscloud/activemq-artemis-operator/issues/52
    podSecurityContext:
      fsGroup: 0
  acceptors:
    - name: anevis-acceptor-all
      protocols: all
      needClientAuth: false
      wantClientAuth: false
      verifyHost: false
      # sslEnabled, expose and exposeMode does not! work for the acceptor. This is bad because there is no ingress creation :(
      # This is because the ssl certificate is evaluated beforehand.
      # You can view this in: activemqartemis_reconciler.go/generateAcceptorsString() + activemqartemis_reconciler.go/generateCommonSSLFlags() <- This is where the error is thrown!
      # There are Options to handle this issue:
          # 1. rewrite the operator so no ssl Key is required such as in the mgmnt console
            # -> Downside: Hacky Workaround, We'd have to check each update throughout on possible clashes
          # 2. use cert-manager as Certificate Authority (supported by awspca/aws-privateca-issuer
            # -> This seems like the 'cleanest' solution however:
            # -> Downside: PRIVATE! CA - we need ACM with public Certificates
          # 3. Forcing the creation of a LoadBalancer (via resourceTemplates) and add ssl manually -> This is what I try in my current attempts
      # exposeMode = mode to expose the acceptor. supported values are route (openshift) and ingress (k8s nginx ing)
      # exposeMode: ingress
      # sslEnabled: true
      # expose: false
      name: anevis-acceptor-all
  console:
    expose: true
    exposeMode: ingress
    name: webconsole
    sslEnabled: false
    # this translates to no specific requirement on the ingress domain
    ingressHost: $(INGRESS_DOMAIN)
  adminUser: artemis
  adminPassword: artemis
