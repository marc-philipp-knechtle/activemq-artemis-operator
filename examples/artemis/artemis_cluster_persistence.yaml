apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: artemis-broker
spec:
  # version: 2.32.0 # uncomment the version, in the hope that the broker choses the most
  resourceTemplates:
      # see https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-1-service-and-ingress-resources/
      # see https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-2-aws-load-balancer-controller/
      # https://aws.amazon.com/it/blogs/containers/setting-up-end-to-end-tls-encryption-on-amazon-eks-with-the-new-aws-load-balancer-controller/
      - selector:
          kind: "Ingress"
        annotations:
            alb.ingress.kubernetes.io/load-balancer-name: artemis-webconsole-lb-$(BROKER_ORDINAL)
            alb.ingress.kubernetes.io/target-type: ip
            alb.ingress.kubernetes.io/scheme: internet-facing
            alb.ingress.kubernetes.io/healthcheck-path: /healthz
      # see https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html
      - selector:
          kind: "Service"
        annotations:
            service.beta.kubernetes.io/aws-load-balancer-type: external
            service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
            service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
  deploymentPlan:
    size: 3
    # (see description in operator.md)
    # = create brokers with persistent Storage, each broker requires 2GB
    # persistenceEnabled requires persistent volumes in the cluster
    #           Error with persistenceEnabled true: 0/3 nodes are available: pod has unbound immediate
    #           PersistentVolumeClaims. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.
    #           -> solved with podSecurityContext.fsGroup
    persistenceEnabled: true
    # <!-- -->
    # messageMigration requires 2+ brokers
    messageMigration: true
    # This enables to use the persistenceEnabled attribute above
    # PodSecurityContext defines the security settings for a pod and all of it's containers
    # fsGroup   special supplemental group where all the containers in a pod belong to. If a container writes a file,
    #           if a container writes a file, the file is created with the GID from fsGroup
    #           Any files created on a mounted volume are owned by the group specified in fsGroup
    #           -> This helps where multiple processes or containers share access to a file/volume
    #           !fsGroup applies to all containers within a pod!
    #           Setting fsGroup: 0 implies that all files created will be owned by the root group
    # -> https://github.com/artemiscloud/activemq-artemis-operator/issues/187 This provided the fix
    # Further discussion here: https://github.com/artemiscloud/activemq-artemis-operator/issues/52
    podSecurityContext:
      fsGroup: 0
  acceptors:
    - name: anevis-acceptor-all
      protocols: all
      sslEnabled: false
      needClientAuth: false
      wantClientAuth: false
      verifyHost: false
      expose: true
      # mode to expose the acceptor. supported values are route and ingress
      # route = exposing with openshift routes
      # ingress = k8s nginx ingress with TLS passthrough
      exposeMode: ingress
      # I don't know the optimal value for ingressHost yet!
      # It is required for acceptors exposed with ingress
      ingressHost: $(INGRESS_DOMAIN)
      name: anevis-acceptor-all
  console:
    expose: true
    name: webconsole
    sslEnabled: false
    exposeMode: ingress
    # this translates to no specific requirement on the ingress domain
    ingressHost: $(INGRESS_DOMAIN)
  adminUser: artemis
  adminPassword: artemis
