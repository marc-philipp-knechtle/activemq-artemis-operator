apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: artemis-broker
spec:
  # version: 2.32.0 # uncomment the version, in the hope that the broker chooses the most recent one!
  resourceTemplates:
      # The new AWS Load Balancer manages AWS Elastic Load Balancers for a Kubernetes cluster.
      # The controller provisions an AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress and
      # an AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer using IP targets on 1.18 or later Amazon EKS clusters.

      # Possible variables in resourceTemplates:
      # ITEM_NAME,
      # CR_NAME = artemis-broker,
      # BROKER_ORDINAL, CR_NAMESPACE, INGRESS_DOMAIN, RES_NAME
      # see https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-1-service-and-ingress-resources/
      # see https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-2-aws-load-balancer-controller/
      # https://aws.amazon.com/it/blogs/containers/setting-up-end-to-end-tls-encryption-on-amazon-eks-with-the-new-aws-load-balancer-controller/
      - selector:
          kind: "Ingress"
        # See list of ingress annotations:
        # https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/
        # Configuration of Application Load Balancer for Ingress
        annotations:
            alb.ingress.kubernetes.io/load-balancer-name: artemis-webconsole-lb-$(BROKER_ORDINAL)
            # Other option: instance
            # ip: target is specified using the ip address
            # instance: target is specified using the ID of the EC2 instance
            # instance is mainly used when backwards compatability is required or if it's required to route traffic
            # through an EC2 instance
            alb.ingress.kubernetes.io/target-type: ip
            # Starting v2.2.0 release, the AWS Load balancer controller provisions an internal NLB by default.
            # To create an internet-facing load balancer, apply the following annotation to your service:
            alb.ingress.kubernetes.io/scheme: internet-facing
            alb.ingress.kubernetes.io/healthcheck-path: /healthz
      # see https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html
      - selector:
          kind: "Service"
        # Configuration of Network Load Balancer
        annotations:
            service.beta.kubernetes.io/aws-load-balancer-name: $(CR_NAME)-$(BROKER_ORDINAL)
            # this is telling k8s 'let AWS handle this load balancer in the cloud' (AWS Load Balancer Controller)
            service.beta.kubernetes.io/aws-load-balancer-type: external
            # see above
            service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
            # see above
            service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
  deploymentPlan:
    size: 3
    # (see description in operator.md)
    # = create brokers with persistent Storage, each broker requires 2GB
    # persistenceEnabled requires persistent volumes in the cluster
    #           Error with persistenceEnabled true: 0/3 nodes are available: pod has unbound immediate
    #           PersistentVolumeClaims. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.
    #           -> solved with podSecurityContext.fsGroup
    persistenceEnabled: true
    # <!-- -->
    # messageMigration requires 2+ brokers
    messageMigration: true
    # This enables to use the persistenceEnabled attribute above
    # PodSecurityContext defines the security settings for a pod and all of it's containers
    # fsGroup   special supplemental group where all the containers in a pod belong to. If a container writes a file,
    #           if a container writes a file, the file is created with the GID from fsGroup
    #           Any files created on a mounted volume are owned by the group specified in fsGroup
    #           -> This helps where multiple processes or containers share access to a file/volume
    #           !fsGroup applies to all containers within a pod!
    #           Setting fsGroup: 0 implies that all files created will be owned by the root group
    # -> https://github.com/artemiscloud/activemq-artemis-operator/issues/187 This provided the fix
    # Further discussion here: https://github.com/artemiscloud/activemq-artemis-operator/issues/52
    podSecurityContext:
      fsGroup: 0
  acceptors:
    - name: anevis-acceptor-all
      protocols: all
      sslEnabled: false
      needClientAuth: false
      wantClientAuth: false
      verifyHost: false
      expose: true
      # mode to expose the acceptor. supported values are route and ingress
      # route = exposing with openshift routes
      # ingress = k8s nginx ingress with TLS passthrough
      exposeMode: ingress
      # I don't know the optimal value for ingressHost yet!
      # It is required for acceptors exposed with ingress
      ingressHost: $(INGRESS_DOMAIN)
      name: anevis-acceptor-all
  console:
    expose: true
    name: webconsole
    sslEnabled: false
    exposeMode: ingress
    # this translates to no specific requirement on the ingress domain
    ingressHost: $(INGRESS_DOMAIN)
  adminUser: artemis
  adminPassword: artemis
